{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Tabular Data - Lecture 1</a>\n",
    "\n",
    "\n",
    "## Exploratory data analysis\n",
    "\n",
    "In this notebook, we go through basic steps of exploratory data analysis (EDA), performing initial data investigations to discover patterns, spot anomalies, and look for insights to inform later ML modeling choices.\n",
    "\n",
    "1. <a href=\"#1\">Read the dataset</a>\n",
    "2. <a href=\"#2\">Overall Statistics</a>\n",
    "3. <a href=\"#3\">Univariate Statistics: Basic Plots</a>\n",
    "4. <a href=\"#4\">Multivariate Statistics: Scatter Plots and Correlations</a>\n",
    "5. <a href=\"#5\">Handling Missing Values</a>\n",
    "    * <a href=\"#51\">Drop columns with missing values</a>\n",
    "    * <a href=\"#52\">Drop rows with missing values</a>\n",
    "    * <a href=\"#53\">Impute (fill-in) missing values with .fillna()</a>\n",
    "    * <a href=\"#54\">Impute (fill-in) missing values with sklearn's SimpleImputer</a>\n",
    "    \n",
    "__Austin Animal Center Dataset__:\n",
    "\n",
    "In this exercise, we are working with pet adoption data from __Austin Animal Center__. We have two datasets that cover intake and outcome of animals. Intake data is available from [here](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Intakes/wter-evkm) and outcome is from [here](https://data.austintexas.gov/Health-and-Community-Services/Austin-Animal-Center-Outcomes/9t4d-g238). \n",
    "\n",
    "In order to work with a single table, we joined the intake and outcome tables using the \"Animal ID\" column and created a single __review.csv__ file. We also didn't consider animals with multiple entries to the facility to keep our dataset simple. If you want to see the original datasets and the merged data with multiple entries, they are available under data/review folder: Austin_Animal_Center_Intakes.csv, Austin_Animal_Center_Outcomes.csv and Austin_Animal_Center_Intakes_Outcomes.csv.\n",
    "\n",
    "__Dataset schema:__ \n",
    "- __Pet ID__ - Unique ID of pet\n",
    "- __Outcome Type__ - State of pet at the time of recording the outcome (0 = not placed, 1 = placed). This is the field to predict.\n",
    "- __Sex upon Outcome__ - Sex of pet at outcome\n",
    "- __Name__ - Name of pet \n",
    "- __Found Location__ - Found location of pet before entered the center\n",
    "- __Intake Type__ - Circumstances bringing the pet to the center\n",
    "- __Intake Condition__ - Health condition of pet when entered the center\n",
    "- __Pet Type__ - Type of pet\n",
    "- __Sex upon Intake__ - Sex of pet when entered the center\n",
    "- __Breed__ - Breed of pet \n",
    "- __Color__ - Color of pet \n",
    "- __Age upon Intake Days__ - Age of pet when entered the center (days)\n",
    "- __Age upon Outcome Days__ - Age of pet at outcome (days)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Read the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's read the dataset into a dataframe, using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "  \n",
    "df = pd.read_csv('data/review/review_dataset.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Overall Statistics</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will look at number of rows, columns and some simple statistics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five rows\n",
    "# NaN means missing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the data types and non-null values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prints basic statistics for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's separate model features and model target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = df.columns.drop('Outcome Type')\n",
    "model_target = 'Outcome Type'\n",
    "\n",
    "print('Model features: ', model_features)\n",
    "print('Model target: ', model_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explore the features set further, figuring out first what features are numerical or categorical. Beware that some integer-valued features could actually be categorical features, and some categorical features could be text features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "numerical_features_all = df[model_features].select_dtypes(include=np.number).columns\n",
    "print('Numerical columns:',numerical_features_all)\n",
    "\n",
    "print('')\n",
    "\n",
    "categorical_features_all = df[model_features].select_dtypes(include='object').columns\n",
    "print('Categorical columns:',categorical_features_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Basic Plots</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this section, we examine our data with plots. Important note: These plots ignore null (missing) values. We will learn how to deal with missing values in the next section.\n",
    "\n",
    "\n",
    "__Bar plots__: These plots show counts of categorical data fields. __value_counts()__ function yields the counts of each unique value. It is useful for categorical variables.\n",
    "\n",
    "First, let's look at the distribution of the model target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[model_target].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__plot.bar()__ addition to the __value_counts()__ function makes a bar plot of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df[model_target].value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now onto the categorical features, exploring number of unique values per feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in categorical_features_all: \n",
    "    print(df[c].value_counts())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the number of unique values (unique IDs for example won't be very useful to visualize, for example), for some categorical features, let's see some bar plot visualizations. For simplicity and speed, here we only show box plots for those features with less than 50 unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for c in categorical_features_all:\n",
    "    if len(df[c].value_counts()) < 50:\n",
    "        print(c)\n",
    "        df[c].value_counts().plot.bar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Histograms:__ Histograms show distribution of numeric data. Data is divided into \"buckets\" or \"bins\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for c in numerical_features_all:\n",
    "    print(c)\n",
    "    df[c].plot.hist(bins=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some histograms the values are heavily placed in the first bin, it is good to check for outliers, either checking the min-max values of those particular features and/or explore value ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in numerical_features_all:\n",
    "    print(c)\n",
    "    print('min:', df[c].min(), 'max:', df[c].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With __value_counts()__ function, we can increase the number of histogram bins to 10 for more bins for a more refined view of the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in numerical_features_all: \n",
    "    print(c)\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any outliers are identified as very likely wrong values, dropping them could improve the numerical values histograms, and later overall model performance. While a good rule of thumb is that anything not in the range of (Q1 - 1.5 IQR) and (Q3 + 1.5 IQR) is an outlier, other rules for removing 'outliers' should be considered as well. For example, removing any values in the upper 1%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in numerical_features_all:\n",
    "    print(c)\n",
    "    \n",
    "    # Drop values below Q1 - 1.5 IQR and beyond Q3 + 1.5 IQR\n",
    "    #Q1 = df[c].quantile(0.25)\n",
    "    #Q3 = df[c].quantile(0.75)\n",
    "    #IQR = Q3 - Q1\n",
    "    #print (Q1 - 1.5*IQR, Q3 + 1.5*IQR)\n",
    "    \n",
    "    #dropIndexes = df[df[c] > Q3 + 1.5*IQR].index\n",
    "    #df.drop(dropIndexes , inplace=True)\n",
    "    #dropIndexes = df[df[c] < Q1 - 1.5*IQR].index\n",
    "    #df.drop(dropIndexes , inplace=True)\n",
    "    \n",
    "    # Drop values beyond 90% of max()\n",
    "    dropIndexes = df[df[c] > df[c].max()*9/10].index\n",
    "    df.drop(dropIndexes , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in numerical_features_all:\n",
    "    print(c)\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the histograms again, with more bins for vizibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in numerical_features_all:\n",
    "    print(c)\n",
    "    df[c].plot.hist(bins=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Scatter Plots and Correlation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "### Scatter plot\n",
    "Scatter plots are simple 2D plots of two numerical variables that can be used to examine the relationship between two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(len(numerical_features_all), len(numerical_features_all), figsize=(16, 16), sharex=False, sharey=False)\n",
    "for i in range(0,len(numerical_features_all)):\n",
    "    for j in range(0,len(numerical_features_all)):\n",
    "        axes[i,j].scatter(x = df[numerical_features_all[i]], y = df[numerical_features_all[j]])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot with Identification\n",
    "\n",
    "We can also add the target values, 0 or 1, to our scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "X1 = df[[numerical_features_all[0], numerical_features_all[1]]][df[model_target] == 0]\n",
    "X2 = df[[numerical_features_all[0], numerical_features_all[1]]][df[model_target] == 1]\n",
    "\n",
    "plt.scatter(X1.iloc[:,0], \n",
    "            X1.iloc[:,1], \n",
    "            s=50, \n",
    "            c='blue', \n",
    "            marker='o', \n",
    "            label='0')\n",
    "\n",
    "plt.scatter(X2.iloc[:,0], \n",
    "            X2.iloc[:,1], \n",
    "            s=50, \n",
    "            c='red', \n",
    "            marker='v', \n",
    "            label='1')\n",
    "\n",
    "plt.xlabel(numerical_features_all[0])\n",
    "plt.ylabel(numerical_features_all[1])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatterplots with identification, can sometimes help identify whether or not we can get good separation between the data points, based on these two numerical features alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix Heatmat\n",
    "We plot the correlation matrix. Correlation scores are calculated for numerical fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=[numerical_features_all[0], numerical_features_all[1]]\n",
    "#print(df[cols].corr())\n",
    "df[cols].corr().style.background_gradient(cmap='tab20c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to scatterplots, but now the correlation matrix values can more clearly pinpoint relationships between the numerical features. Correlation values of -1 means perfect negative correlation, 1 means perfect positive correlation, and 0 means there is no relationship between the two numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A fancy example using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a large random dataset\n",
    "rs = np.random.RandomState(33)\n",
    "d = pd.DataFrame(data=rs.normal(size=(100, 26)),\n",
    "                 columns=list(ascii_letters[26:]))\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = d.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, more exploratory data analysis might reveal other important hidden atributes and/or relationships of the model features considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Handling Missing Values</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "  * <a href=\"#51\">Drop columns with missing values</a>\n",
    "  * <a href=\"#52\">Drop rows with missing values</a>\n",
    "  * <a href=\"#53\"> Impute (fill-in) missing values with .fillna()</a>\n",
    "  * <a href=\"#54\"> Impute (fill-in) missing values with sklearn's SimpleImputer</a>\n",
    "\n",
    "Let's first check the number of missing (nan) values for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a few options dealing with missing values, when there are values missing on many features, both numerical and categorical types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"51\">Drop columns with missing values</a>\n",
    "(<a href=\"#5\">Go to Handling Missing Values</a>)\n",
    "\n",
    "We can drop some feautures/columns if we think there is significant amount of missing data in those features. Here we \n",
    "are dropping features having more than 20% missing values.\n",
    "\n",
    "__Hint:__ You can also use __inplace=True__ parameter to drop features inplace without assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2/10\n",
    "print((df.isna().sum()/len(df.index)))\n",
    "columns_to_drop = df.loc[:,list(((df.isna().sum()/len(df.index))>=threshold))].columns    \n",
    "print(columns_to_drop)\n",
    "\n",
    "df_columns_dropped = df.drop(columns_to_drop, axis = 1)  \n",
    "df_columns_dropped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_dropped.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_dropped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the reduced size of the dataset features. This can sometimes lead to underfitting models -- not having enough features to build a good model able to capture the pattern in the dataset, especially when dropping features that are essential to the task at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"52\">Drop rows with missing values</a>\n",
    "(<a href=\"#5\">Go to Handling Missing Values</a>)\n",
    "\n",
    "Here, we simply drop rows that have at least one missing value. There are other drop options to explore, depending on specific problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_dropped = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the missing values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_missing_dropped.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why did lose so many records?  What should we have done differently?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can dramatically reduce the number of data samples. This can sometimes lead to overfitting models -- especially when the number of features is greater or comparable to the number of data samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"53\">Impute (fill-in) missing values with .fillna()</a>\n",
    "(<a href=\"#5\">Go to Handling Missing Values</a>)\n",
    "\n",
    "Rather than dropping rows (data samples) and/or columns (features), another strategy to deal with missing values would be to actually complete the missing values with new values: imputation of missing values.\n",
    "\n",
    "__Imputing Numerical Values:__ The easiest way to impute numerical values is to get the __average (mean) value__ for the corresponding column and use that as the new value for each missing record in that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute numerical features by using the mean per feature to replace the nans\n",
    "\n",
    "# Assign our df to a new df \n",
    "df_imputed = df.copy()\n",
    "print(df_imputed[numerical_features_all].isna().sum())\n",
    "\n",
    "# Impute our two numerical features with the means. \n",
    "df_imputed[numerical_features_all] = df_imputed[numerical_features_all].fillna(df_imputed[numerical_features_all].mean())\n",
    "\n",
    "print(df_imputed[numerical_features_all].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Imputing Categorical Values:__ We can impute categorical values by getting the most common (mode) value for the corresponding column and use that as the new value for each missing record in that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute categorical features by using the mode per feature to replace the nans\n",
    "\n",
    "# Assign our df to a new df \n",
    "df_imputed_c = df.copy()\n",
    "print(df_imputed_c[categorical_features_all].isna().sum())\n",
    "\n",
    "for c in categorical_features_all:\n",
    "    # Find the mode per each feature\n",
    "    mode_impute = df_imputed_c[c].mode()\n",
    "    print(c, mode_impute)\n",
    "\n",
    "    # Impute our categorical features with the mode\n",
    "    # \"inplace=True\" parameter replaces missing values in place (no need for left handside assignment)\n",
    "    \n",
    "    # df_imputed_c[c].fillna(False, inplace=True)\n",
    "    df_imputed_c[c].fillna(mode_impute, inplace=True)\n",
    "    # instead of False, I think this should be the value your want to imput with, in this case \"mode_impute\"\n",
    "    \n",
    "print(df_imputed_c[categorical_features_all].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create a new category, such as \"Missing\", for alll or elected categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute categorical features by using a placeholder value\n",
    "\n",
    "# Assign our df to a new df \n",
    "df_imputed = df.copy()\n",
    "print(df_imputed[categorical_features_all].isna().sum())\n",
    "\n",
    "# Impute our categorical features with a new category named \"Missing\". \n",
    "df_imputed[categorical_features_all]= df_imputed[categorical_features_all].fillna(\"Missing\")\n",
    "\n",
    "print(df_imputed[categorical_features_all].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"54\">Impute (fill-in) missing values with sklearn's __SimpleImputer__</a>\n",
    "(<a href=\"#5\">Go to Handling Missing Values</a>)\n",
    "\n",
    "A more elegant way to implement imputation is using sklearn's __SimpleImputer__, a class implementing .fit() and .transform() methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute numerical columns by using the mean per column to replace the nans\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assign our df to a new df\n",
    "df_sklearn_imputed = df.copy()\n",
    "print(df_sklearn_imputed[numerical_features_all].isna().sum())\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_sklearn_imputed[numerical_features_all] = imputer.fit_transform(df_sklearn_imputed[numerical_features_all])\n",
    "\n",
    "print(df_sklearn_imputed[numerical_features_all].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute categorical columns by using the mode per column to replace the nans\n",
    "\n",
    "# Pick some categorical features you desire to impute with this approach\n",
    "categoricals_missing_values = df[categorical_features_all].loc[:,list(((df[categorical_features_all].isna().sum()/len(df.index)) > 0.0))].columns    \n",
    "columns_to_impute = categoricals_missing_values[1:3]\n",
    "print(columns_to_impute)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assign our df to a new df\n",
    "df_sklearn_imputer = df.copy()\n",
    "print(df_sklearn_imputer[columns_to_impute].isna().sum())\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df_sklearn_imputer[columns_to_impute] = imputer.fit_transform(df_sklearn_imputer[columns_to_impute])\n",
    "\n",
    "print(df_sklearn_imputer[columns_to_impute].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute categorical columns by using a placeholder \"Missing\"\n",
    "\n",
    "# Pick some categorical features you desire to impute with this approach\n",
    "categoricals_missing_values = df[categorical_features_all].loc[:,list(((df[categorical_features_all].isna().sum()/len(df.index)) > 0.0))].columns    \n",
    "columns_to_impute = categoricals_missing_values[1:3]\n",
    "print(columns_to_impute)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assign our df to a new df\n",
    "df_sklearn_imputer = df.copy()\n",
    "print(df_sklearn_imputer[columns_to_impute].isna().sum())\n",
    "\n",
    "imputer = SimpleImputer(strategy='constant', fill_value = \"Missing\")\n",
    "df_sklearn_imputer[columns_to_impute] = imputer.fit_transform(df_sklearn_imputer[columns_to_impute])\n",
    "\n",
    "print(df_sklearn_imputer[columns_to_impute].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
