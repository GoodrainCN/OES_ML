{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Run the following cell to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Supervised Learning: Logistic Regression\n",
    "\n",
    "---\n",
    "<a class=\"anchor\" id=\"log\"></a>\n",
    "\n",
    "Sources: \n",
    "\n",
    "https://towardsdatascience.com/introduction-to-logistic-regression-66248243c148\n",
    "\n",
    "\n",
    "https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n",
    "\n",
    "Logistic Regression is used when the dependent variable (target) is categorical. For example,\n",
    "\n",
    "-to predict whether an email is spam (1) or (0)\n",
    "\n",
    "-Whether the tumor is malignant (1) or not (0)\n",
    "\n",
    "There are several types of logistic regressions:\n",
    "\n",
    "1.Binary Logistic Regression\n",
    "\n",
    "The categorical response has only two 2 possible outcomes. Example: Spam or Not\n",
    "\n",
    "2.Multinomial Logistic Regression\n",
    "\n",
    "Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n",
    "\n",
    "3.Ordinal Logistic Regression\n",
    "\n",
    "Three or more categories with ordering. Example: Movie rating from 1 to 5\n",
    "\n",
    "We'll stick with a basic Binary Logistic Regression example for now. Suppose that you would like to predict from the number of positive cancerous lymph nodes, whether a patient will survive or not five years from now:\n",
    "\n",
    "<img src=\"images/log1.png\" width=\"400\">\n",
    "\n",
    "\n",
    "Or maybe taking into account two explanatory variables, age and nodes, whether a patient will survive or not:\n",
    "\n",
    "<img src=\"images/log2.png\" width=\"300\">\n",
    "\n",
    "\n",
    "We could use a linear regression, and label everyone above the line y = 1/2 as deceased and everyone below the line as survived. However, we would misclassify people:\n",
    "\n",
    "<img src=\"images/log3.png\" width=\"400\">\n",
    "\n",
    "A better solution would be to find a \"curvier\" function that has a steeper ascent and a range of only (0,1). The Sigmoid function, also known as the logistic function, will do:\n",
    "\n",
    "\n",
    "<img src=\"images/log4.png\" width=\"400\">\n",
    "\n",
    "When using linear regression, recall that our hypothesis function was $h(\\theta_0,\\theta_1) = \\theta_0+\\theta_1x$. \n",
    "\n",
    "Using logistic regression, our function will be: $h(\\theta) = \\frac{1}{1+e^{-(\\theta_0+\\theta_1x)}}$.\n",
    "\n",
    "If this function returns a value greater than 0.5 based on an input x (the number of positive nodes), then we will label the patient as deceased. Otherwise, we'll label them as survived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We learned about the cost function J(θ) in linear regression. The cost function represents optimization objective i.e. we create a cost function and minimize it so that we can develop an accurate model with minimum error:\n",
    "$J(\\theta) = \\frac{1}{2n} \\sum_{i=1}^n(h_{\\theta}(x_i)-y^{(i)})^2$\n",
    "\n",
    "If we try to use the cost function of the linear regression in Logistic Regression then it would be of no use as it would end up being a non-convex function with many local minimums, in which it would be very difficult to minimize the cost value and find the global minimum.\n",
    "\n",
    "<img src=\"images/log5.png\" width=\"400\">\n",
    "\n",
    "Instead, the cost function for Logistic regression will be:\n",
    "\n",
    "<img src=\"images/log6.png\" width=\"400\">\n",
    "\n",
    "<img src=\"images/log7.png\" width=\"600\">\n",
    "\n",
    "The above two functions can be compressed into a single function:\n",
    "\n",
    "<img src=\"images/log8.png\" width=\"600\">\n",
    "\n",
    "Now the question arises, how do we reduce the cost value? Well, this can be done by using Gradient Descent. The main goal of Gradient Descent is to minimize the cost value, i.e., minimize J(θ).\n",
    "\n",
    "When we take the partial derivative of the cost function and plug it into our gradient descent formula, once again we get the familiar algorithm:\n",
    "\n",
    "<img src=\"images/log9.png\" width=\"600\">\n",
    "\n",
    "You’ll note the summation term is the exact same form as the one you get when deriving gradient descent for linear regression. It is different however because in this case the hypothesis is a logistic function, not a linear one. (And as a tiny extra reason, because we no longer see the division by m out front.)\n",
    "\n",
    "<img src=\"images/log10.png\" width=\"600\">\n",
    "\n",
    "How do we get to the same form as the linear regression case??? This is NOT obvious! You will do the derivation for homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By the way - you will see the logistic regression equation written in several equivalent forms:\n",
    "\n",
    "\n",
    "$\\log(\\frac{h(\\theta)}{1-h(\\theta)}) = \\theta_0+\\theta_1x$\n",
    "\n",
    "\n",
    "OR:\n",
    "\n",
    "\n",
    "$h(\\theta) = \\frac{1}{1+e^{-(\\theta_0+\\theta_1x)}}$\n",
    "\n",
    "OR:\n",
    "\n",
    "$h(\\theta) = \\frac{e^{\\theta_0+\\theta_1x}}{1+e^{\\theta_0+\\theta_1x}}$\n",
    "\n",
    "Note: if you need more explanation on Logistic Regression, watch these two Andrew Ng videos:\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/lecture/1XG8G/cost-function\n",
    "\n",
    "\n",
    "https://www.coursera.org/learn/machine-learning/lecture/MtEaZ/simplified-cost-function-and-gradient-descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We'll do the simplest example possible right now, a binary classifier for dogs with just one predictor variable (weight). Let's read the data in, where 1 corresponds to healthy and 0 corresponds to unhealthy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "df = pd.read_csv(\"data/dogweights.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's create our logistic regression model and calculate its score (which returns the mean accuracy on the given test data and labels). We see that out of 20 dogs, it classifies 85% correctly, meaning only 3 dogs got classified incorrectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X = df.drop(columns = [\"health status\"])\n",
    "y = df[\"health status\"]\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X, y)\n",
    "print(model.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can view the $\\theta_0$ and $\\theta_1$ by typing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Homework 1 - Calculus Derivation  -- OPTIONAL\n",
    "\n",
    "Using multivariable calculus and properties of logarithms, show that linear regression and logistic regression have nearly the same gradient descent formulas. If you need help, consult this source:\n",
    "\n",
    "\n",
    "https://math.stackexchange.com/questions/477207/derivative-of-cost-function-for-logistic-regression?newreg=28a1a02102d9489caad408b4335adfc2\n",
    "\n",
    "\n",
    "### Homework 2 - Logistic Regression Gradient Descent\n",
    "\n",
    "Edit your previous gradient descent algorithm to now find the optimal parameters for a logistic regression with one explanatory variable.\n",
    "\n",
    "Note that the two main things that you will have to edit are a.) the $h(\\theta)$ function and b.) take the division by m (the number of points) out of your $\\theta_j$ update.\n",
    "\n",
    "To check your work, if you type ```gradient_descent(X,y, alpha= 5e-5, num_steps = 100000)``` with an initial guess of $\\theta_0 = 0$ and $\\theta_1 = 0$, then it should take a long time to run but it will end up giving you out approximately $\\theta_0 = 3.67, \\theta_1 = -0.03$, which isn't quite there yet but relatively close to the sci-kit learn values above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# insert HW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### An improvement\n",
    "Note: Our algorithm is slow in part because it does not utilizie the built-in numpy data structures that speed things up. Note how much faster this algorithm runs by utilizing numpy arrays and the dot product of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def h(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "def logistic_regression(features, target, alpha, num_steps):\n",
    "    intercept = np.ones((features.shape[0], 1))\n",
    "    features = np.hstack((intercept, features))\n",
    "        \n",
    "    thetas = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, thetas)\n",
    "        predictions = h(scores)\n",
    "        \n",
    "        # Update weights with gradient\n",
    "        output_error_signal = predictions - target\n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        thetas -= alpha * gradient\n",
    "\n",
    "        if step % 1000 == 0:\n",
    "            print(step, thetas)\n",
    "            \n",
    "    print(thetas)\n",
    "        \n",
    "    return thetas\n",
    "\n",
    "X = df[\"weight\"]\n",
    "y = df[\"health status\"]\n",
    "X = np.array(X)[:, np.newaxis]\n",
    "\n",
    "logistic_regression(X, y, alpha = 5e-5, num_steps = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A big improvement. \n",
    "\n",
    "Note that the the code above may have taken much quicker than the one before it but it's still MUCH slower and less accurate than the built-in sci-kit learn method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "X = df.drop(columns = [\"health status\"])\n",
    "y = df[\"health status\"]\n",
    "\n",
    "model = LogisticRegression(solver=\"lbfgs\")\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}