{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read in the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Linear Algebra Python Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.Let's see what the very important numpy and linalg packages can help us with. We can create matrices and multiply them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A = np.matrix([[3,0],[8,-1]])\n",
    "b = np.matrix([[1],[2]])\n",
    "A*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.We can calculate the inverse, tranpose, and the determinant of a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#inverse:\n",
    "print(linalg.inv(A))\n",
    "\n",
    "print()\n",
    "\n",
    "#transpose:\n",
    "print(A.T)\n",
    "\n",
    "print()\n",
    "\n",
    "#determinant:\n",
    "print(linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.We can solve the system $Ax=b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4.We can find the least squares solution and projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#least squares\n",
    "A=np.matrix([[1,1],[1,2],[1,3]])\n",
    "b=np.matrix([[1],[2],[2]])\n",
    "\n",
    "X=linalg.inv(A.T*A)*A.T*b\n",
    "print(X) # solution\n",
    "print()\n",
    "print(A*X) # projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5.We can also calculate the eigenvalues and eigenvectors of a matrix, which we'll get to more later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A=np.matrix([[3,0],[8,-1]])\n",
    "\n",
    "#eigenvalues:\n",
    "print(linalg.eigvals(A))\n",
    "\n",
    "#eigenvalues and eigenvectors:\n",
    "print(linalg.eig(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note that the above output is read as $ \\lambda_1 = -1, \\lambda_2 = 3$ with eigenvectors $v_1 = [0,1],v_2=[0.447,0.894]$, which are the normalized versions of what you get from obtaining $v_1 = [0,1],v_2=[1,2]$ by hand.\n",
    "\n",
    "I must admit that the output you get from typing ```eigenvectors(([[3,0],[8,-1]])``` into wolfram alpha is nicer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### College Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1.Read in the US News & World Report 2013 College Rankings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/collegedata.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2.Drop Grinnell since it does not contain SAT info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df[df['College'] != 'grinnell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3.Save the Score column as series b and save the rest of the dataframe (except for the college and score columns) as A_dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "b = df['Score']\n",
    "A_dataframe = df.drop(columns=['College', 'Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4.Convert the dataframes to numpy matrices and then tranpose vector b so that the shapes are 25x14 and 25x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "A=np.matrix(A_dataframe)\n",
    "b=np.matrix(b)\n",
    "b = b.T\n",
    "print(A.shape, b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5.Apply the least squares transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X=linalg.inv(A.T*A)*(A.T)*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "6.Print the weights in decending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categories = A_dataframe.columns                   # column names\n",
    "\n",
    "tuples = []                                        # create tuples containing the category weights and names\n",
    "for i in range(len(categories)):\n",
    "    tuples.append((X[i][0,0], categories[i]))\n",
    "    \n",
    "tuples.sort(reverse = True)                        # sort in decending order\n",
    "\n",
    "for i in range(len(categories)):                   # print the output\n",
    "    print(tuples[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "7.What questions or observations do you have about the importance of these categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#insert here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "8.Print Colby's actual score and predicted score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#get colby's info\n",
    "colby = df[df['College'] == 'colby']\n",
    "\n",
    "#apply the least squares projection to colby's info\n",
    "colby = colby.drop(columns = ['College', 'Score'])\n",
    "colby = np.matrix(colby)\n",
    "colby\n",
    "\n",
    "projection = colby*X\n",
    "\n",
    "#print the predicted and actual ranking\n",
    "print('predicted ranking:', projection[0,0])\n",
    "print('colby actual ranking:', df[df['College'] == 'colby']['Score'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The average absolute error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "error = 0\n",
    "for i in range(len(A)):\n",
    "    projection = A[i][:]*X\n",
    "    newerror = abs(projection[0,0] - b[i,0])\n",
    "    error = error + newerror\n",
    "    \n",
    "print('Average Absolute Prediction Error:', error/len(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### A Discussion of Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "SAT Scores are on a much different scale than the other variables, so perhaps we should scale first? We have several options. We do NOT want to use the Standard scaler in this case because it causes the square matrix  $A^T A$ to have a determinant very close to zero, making it a nearly non-invertible matrix, which is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(A_dataframe)\n",
    "\n",
    "A = np.matrix(scaler.transform(A_dataframe))\n",
    "\n",
    "print(linalg.det(linalg.inv(A.T*A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can try using the MinMax scaler instead, and this will no longer give us a zero determinat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(A_dataframe)\n",
    "\n",
    "A = np.matrix(scaler.transform(A_dataframe))\n",
    "\n",
    "print(linalg.det(linalg.inv(A.T*A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "However, it still does not give us predictions as good as the non-scaled version. For example, the weightings are wackier and Colby's prediction is worse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X=linalg.inv(A.T*A)*(A.T)*b\n",
    "\n",
    "categories = A_dataframe.columns                   # column names\n",
    "\n",
    "tuples = []                                        # create tuples containing the category weights and names\n",
    "for i in range(len(categories)):\n",
    "    tuples.append((X[i][0,0], categories[i]))\n",
    "    \n",
    "tuples.sort(reverse = True)                        # sort in decending order\n",
    "\n",
    "for i in range(len(categories)):                   # print the output\n",
    "    print(tuples[i])\n",
    "    \n",
    "    \n",
    "#get colby's info\n",
    "colby = df[df['College'] == 'colby']\n",
    "\n",
    "#apply the least squares projection to colby's info\n",
    "colby = colby.drop(columns = ['College', 'Score'])\n",
    "\n",
    "\n",
    "\n",
    "colby = scaler.transform(colby)   # don't scale because it gives worse results\n",
    "colby = np.matrix(colby)\n",
    "colby\n",
    "\n",
    "projection = colby*X\n",
    "\n",
    "#print the predicted and actual ranking\n",
    "print('predicted ranking:', projection[0,0])\n",
    "print('colby actual ranking:', df[df['College'] == 'colby']['Score'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The average error is also worse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "error = 0\n",
    "for i in range(len(A)):\n",
    "    projection = A[i][:]*X\n",
    "    newerror = abs(projection[0,0] - b[i,0])\n",
    "    error = error + newerror\n",
    "    \n",
    "print('Average Absolute Prediction Error:', error/len(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In summary, when using Ordinary Least Squares to solve for the closed form solution, feature scaling will NOT be necessary; in fact, it may make our predictions worse.\n",
    "\n",
    "The exception is when you apply regularization (L1/L2 Ridge, Lasso, etc.) Then you should use feature scaling. However, in the above examples, we have not applied any regularization.\n",
    "\n",
    "In the past, we used feature scaling for gradient descent in order to help the solution converge in a shorter period of time.\n",
    "\n",
    "Speaking of gradient descent..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ordinary Least Squares vs. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There are two ways to solve for an optimal regression solution. You can solve it via the analytical solution (OLS) or via an iterative algorithm such as gradient descent.\n",
    "\n",
    "1.**Similarities** between the two methods:\n",
    "\n",
    "- Both can be applied to linear regression models.\n",
    "- Both are minimizing the sum of the squared residuals $$\\sum_{i=1}^n(y^{(i)}_{\\text{predicted}}-y^{(i)}_{\\text{actual}})^2$$\n",
    "- Both work for multivariate problems.\n",
    "\n",
    "2.**Differences** between the two methods:\n",
    "\n",
    "OLS directly calculates the solution by solving for the system of equations generated when setting all partial derivatives = 0. The whole process is analytical and generates a closed form solution.\n",
    "\n",
    "In contrast, gradient descent starts from guessing the local min and proceeds by taking small steps along the direction of the steepest descent. It's numerical and iterative and when the step size is small enough, one expects the updated guess to approach the real local min (converge to the least squared solution). In addition, gradient descent is able to tackle a wider array of problems that are analytically unsolvable.\n",
    "\n",
    "The OLS closed-form solution may (should) be preferred for “smaller” datasets – if computing (a “costly”) matrix inverse is not a concern. For very large datasets, or datasets where the inverse of $X^T X$ may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the GD or SGD approaches are to be preferred. Here's a good article going into more detail...\n",
    "\n",
    "https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}