{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### k-Nearest Neighbors\n",
    "\n",
    "Run the following cell to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Sources: \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/\n",
    "\n",
    "https://medium.com/analytics-vidhya/why-is-scaling-required-in-knn-and-k-means-8129e4d88ed7\n",
    "\n",
    "https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n",
    "\n",
    "https://towardsdatascience.com/implement-k-nearest-neighbors-classification-algorithm-c99be8f14052\n",
    "\n",
    "https://www.geeksforgeeks.org/k-nearest-neighbours/\n",
    "\n",
    "The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\n",
    "\n",
    "<img src=\"images/k1.png\" width=\"400\">\n",
    "\n",
    "As an example, consider the following table of data points containing two features:\n",
    "\n",
    "<img src=\"images/k2.png\" width=\"400\">\n",
    "\n",
    "Now, given another set of data points (also called testing data), allocate these points a group by analyzing the training set. Note that the unclassified points are marked as ‘White’.\n",
    "\n",
    "<img src=\"images/k3.png\" width=\"400\">\n",
    "\n",
    "If we plot these points on a graph, we may be able to locate some clusters or groups. Now, given an unclassified point, we can assign it to a group by observing what group its nearest neighbors belong to. This means a point close to a cluster of points classified as ‘Red’ has a higher probability of getting classified as ‘Red’.\n",
    "\n",
    "Intuitively, we can see that the first point (2.5, 7) should be classified as ‘Green’ and the second point (5.5, 4.5) should be classified as ‘Red’.\n",
    "\n",
    "***k-Nearest Neighbor algorithm**\n",
    "\n",
    "Let m be the number of training data samples. Let p be an unknown point.\n",
    "\n",
    "1. Store the training samples in an array of data points.\n",
    "\n",
    "2. Calculate the distance between each of the training data points and the point p.\n",
    "\n",
    "3. Take the k-smallest distances obtained. Each of these distances corresponds to an already classified data point.\n",
    "\n",
    "4. Return the majority label among among these k points.\n",
    "\n",
    "K can be kept as an odd number so that we can calculate a clear majority in the case where only two groups are possible (e.g. Red/Blue). \n",
    "\n",
    "## How do we choose the factor of k?\n",
    "\n",
    "Notice the boundary for these various k values:\n",
    "\n",
    "<img src=\"images/k5.png\" width=\"600\">\n",
    "\n",
    "If you watch carefully, you can see that the boundary becomes smoother with increasing value of K. With K increasing to infinity it finally becomes all blue or all red depending on the total majority.  The training error rate and the validation error rate are two parameters we need to access on different K-values. Following is the curve for the training error rate with varying value of K :\n",
    "\n",
    "<img src=\"images/k6.png\" width=\"600\">\n",
    "\n",
    "As you can see, the error rate at K=1 is always zero for the training sample. This is because the closest point to any training data point is itself.Hence the prediction is always accurate with K=1. If the validation error curve would have been similar, our choice of K would have been 1. Following is the validation error curve with varying value of K:\n",
    "\n",
    "<img src=\"images/k7.png\" width=\"600\">\n",
    "\n",
    "This makes the story more clear. At K=1, we we're overfitting the boundaries. Hence, error rate initially decreases and reaches a minima. After the minima point, it then increase with increasing K. To get the optimal value of K, you can segregate the training and validation from the initial dataset. Now plot the validation error curve to get the optimal value of K. This value of K should be used for all predictions.\n",
    "\n",
    "### Scaling the Data\n",
    "For the kNN algorithm, it is essential that we preprocess the data by scaling it. kNN is a distance-based algorithm and all such distance based algorithms are affected by the scale of the variables. For example, consider the issues inherent in calculating the distance between the points in the graph below, where the $x_1$ values are on a much smaller scale than the $x_2$ values:\n",
    "\n",
    "<img src=\"images/k10.png\" width=\"500\">\n",
    "\n",
    "Consider a concrete example in which your data has an age variable which tells about the age of a person in years and an income variable which tells the monthly income of the person in rupees:\n",
    "\n",
    "<img src=\"images/k8.png\" width=\"300\">\n",
    "\n",
    "Here the Age of the person ranges from 25 to 40 whereas the income variable ranges from 50,000 to 110,000. The Euclidean distance between observation 1 and 2 will be given as:\n",
    "\n",
    "$\\text{Euclidean Distance} = \\sqrt{(100000–80000)^2 + (30–25)^2}$\n",
    "\n",
    "which will come out to be around 20000.000625. \n",
    "\n",
    "It can be noted here that the high magnitude of income affected the distance between the two points. This will impact the performance of all distance based model as it will give higher weightage to variables which have higher magnitude (income in this case).\n",
    "\n",
    "\n",
    "We do not want our algorithm to be affected by the magnitude of these variables. The algorithm should not be biased towards variables with higher magnitude. To overcome this problem, we can bring down all the variables to the same scale. One of the most common technique to do so is normalization where we calculate the mean and standard deviation of the variable. Then for each observation, we subtract the mean and then divide by the standard deviation of that variable:\n",
    "\n",
    "$x_{\\text{norm}} = \\frac{x - u}{s}$.\n",
    "\n",
    "\n",
    "\n",
    "Apart from normalization, there are other methods too to bring down all the variables to the same scale. For example: Min-Max Scaling. Here the scaling is done using the following formula:\n",
    "\n",
    "$x_{\\text{norm}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}$.\n",
    "\n",
    "For now, we will be focusing on the first type. You can try min-max scaling as well. Let’s see how normalization can bring down these variables to same scale and hence improve the performance of these distance based algorithms. If we normalize the above data, it will look like:\n",
    "\n",
    "<img src=\"images/k9.png\" width=\"300\">\n",
    "\n",
    "Let’s again calculate the Euclidean distance between observation 1 and 2:\n",
    "\n",
    "$\\text{Euclidean Distance} = \\sqrt{(0.608+0.260)^2 + (-0.447+1.192)^2}$\n",
    "\n",
    "This time the distance is around 1.1438. We can clearly see that the distance is not biased towards the income variable. It is now giving similar weight to both the variables. Hence, it is always advisable to bring all the features to the same scale for applying distance based algorithms like KNN or K-Means (which we'll get to next).\n",
    "\n",
    "\n",
    "### Heart Example\n",
    "\n",
    "We are going to use a cleaned, reduced dataset of the Cleveland data located here:\n",
    "\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "\n",
    "Ourr goal will be to predict the presence of heart disease in a patient or not.\n",
    "\n",
    "\n",
    "The dataset contains following features:\n",
    "- age — age in years \n",
    "- sex — (1 = male; 0 = female) \n",
    "- cp — chest pain type \n",
    "- trestbps — resting blood pressure (in mm Hg on admission to the hospital) \n",
    "- chol — serum cholestoral in mg/dl \n",
    "- fbs — (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) \n",
    "- restecg — resting electrocardiographic results \n",
    "- thalach — maximum heart rate achieved \n",
    "- exang — exercise induced angina (1 = yes; 0 = no) \n",
    "- oldpeak — ST depression induced by exercise relative to rest \n",
    "- slope — the slope of the peak exercise ST segment \n",
    "- ca — number of major vessels (0–3) colored by flourosopy \n",
    "- thal — 3 = normal; 6 = fixed defect; 7 = reversable defect \n",
    "- target — have disease or not (1=yes, 0=no)\n",
    "\n",
    "Let's load the dataset in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>54.366337</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>0.966997</td>\n",
       "      <td>131.623762</td>\n",
       "      <td>246.264026</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.528053</td>\n",
       "      <td>149.646865</td>\n",
       "      <td>0.326733</td>\n",
       "      <td>1.039604</td>\n",
       "      <td>1.399340</td>\n",
       "      <td>0.729373</td>\n",
       "      <td>2.313531</td>\n",
       "      <td>0.544554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.082101</td>\n",
       "      <td>0.466011</td>\n",
       "      <td>1.032052</td>\n",
       "      <td>17.538143</td>\n",
       "      <td>51.830751</td>\n",
       "      <td>0.356198</td>\n",
       "      <td>0.525860</td>\n",
       "      <td>22.905161</td>\n",
       "      <td>0.469794</td>\n",
       "      <td>1.161075</td>\n",
       "      <td>0.616226</td>\n",
       "      <td>1.022606</td>\n",
       "      <td>0.612277</td>\n",
       "      <td>0.498835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>274.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age         sex          cp    trestbps        chol         fbs  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
       "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
       "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
       "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
       "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
       "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
       "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
       "\n",
       "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
       "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
       "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
       "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
       "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
       "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
       "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
       "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
       "\n",
       "             thal      target  \n",
       "count  303.000000  303.000000  \n",
       "mean     2.313531    0.544554  \n",
       "std      0.612277    0.498835  \n",
       "min      0.000000    0.000000  \n",
       "25%      2.000000    0.000000  \n",
       "50%      2.000000    1.000000  \n",
       "75%      3.000000    1.000000  \n",
       "max      3.000000    1.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/heart.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's do a test/train:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### A 71 year old male enters the ER.  He has a resting blood pressure of 134 and cholesterol of 244 and complains of chest pain of type 2.  Some additional testing determines the following:\n",
    "- fbs: 1\n",
    "- resting ecg: 1\n",
    "- thalach: 159\n",
    "- exang: 1\n",
    "- old peak: .9\n",
    "- slope: 2\n",
    "- ca: 2\n",
    "- thal: 6\n",
    "\n",
    "###  Does your model predict that this person has heart disease?  What does your model say is the probability of him having heart disease? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['target'])\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's scale the data according to the formula $z = \\frac{x - u}{s}$.\n",
    "\n",
    "Also, [here is a good article](https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe) about why we use .fit_tranform on the training data but use .transform on the test data:\n",
    "\n",
    "https://towardsdatascience.com/what-and-why-behind-fit-transform-vs-transform-in-scikit-learn-78f915cf96fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's run our kNN algorithm with k=1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7894736842105263\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about k=5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8157894736842105\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about k=1 to k=20?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7894736842105263\n",
      "2 0.7763157894736842\n",
      "3 0.8421052631578947\n",
      "4 0.8421052631578947\n",
      "5 0.8157894736842105\n",
      "6 0.8552631578947368\n",
      "7 0.868421052631579\n",
      "8 0.868421052631579\n",
      "9 0.8552631578947368\n",
      "10 0.8552631578947368\n",
      "11 0.8421052631578947\n",
      "12 0.8421052631578947\n",
      "13 0.8289473684210527\n",
      "14 0.8289473684210527\n",
      "15 0.8289473684210527\n",
      "16 0.8157894736842105\n",
      "17 0.8289473684210527\n",
      "18 0.8157894736842105\n",
      "19 0.8157894736842105\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,20):\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(i, model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It appears that k=7 may be best. However, we've got a problem:\n",
    "\n",
    "**Problem:** using a single train-test split means that errors from the original test set are used to select the correct value of `n_neighbors`. This is not OK! Performing any part of the modeling process other than evaluation of the final error metric on the test set is overfitting.\n",
    "\n",
    "**Solution:** create either an additional split of the training set (train_small and validation) or use cross-validation on the training set, which is really the same thing as a train_small/validation split.\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "Here is what cross-validation on the training set looks like for a single value of `n_neighbors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76315789 0.84210526 0.82666667]\n",
      "0.8106432748538012\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(model, X_train, y_train, \n",
    "                     cv=3, scoring='accuracy')\n",
    "\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This can be repeated for multiple values of k and the results used to choose the best value of n_neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.709415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.771053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.771053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.793099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.810643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.819474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.806199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.810585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.832632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.832632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.841462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.832749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.837076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.837018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.832690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.841462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.841462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.841520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.837018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy\n",
       "k           \n",
       "1   0.709415\n",
       "2   0.771053\n",
       "3   0.771053\n",
       "4   0.793099\n",
       "5   0.810643\n",
       "6   0.819474\n",
       "7   0.806199\n",
       "8   0.810585\n",
       "9   0.832632\n",
       "10  0.832632\n",
       "11  0.841462\n",
       "12  0.832749\n",
       "13  0.837076\n",
       "14  0.837018\n",
       "15  0.832690\n",
       "16  0.841462\n",
       "17  0.841462\n",
       "18  0.841520\n",
       "19  0.837018"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_scores = []\n",
    "\n",
    "for k in range(1, 20):\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    \n",
    "    k_scores.append((k, scores.mean()))\n",
    "    \n",
    "k_scores = pd.DataFrame(k_scores, columns=['k', 'accuracy']).set_index('k')\n",
    "k_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can find the maximum accuracy of the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.84152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy\n",
       "k           \n",
       "18   0.84152"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_max = k_scores[k_scores.accuracy==max(k_scores.accuracy)]\n",
    "k_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = k_scores.plot()\n",
    "ax.set(xlabel='Number of Neighbors', ylabel='Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In our case, we may have more than one contender to check. We can measure the final error value on each of these contenders on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8157894736842105\n"
     ]
    }
   ],
   "source": [
    "for k in k_max.index.values:\n",
    "    # get the model\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # Have to fit on training data again as \n",
    "    # cross-validation does not return fitted model\n",
    "\n",
    "    model = model.fit(X_train, y_train)\n",
    "\n",
    "    print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question:** Can this process be simplified and adapted to search over multiple parameter options?\n",
    "\n",
    "**Solution:** Use grid search with cross-validation.\n",
    "\n",
    "## Parameter tuning using `GridSearchCV`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 18} KNeighborsClassifier(n_neighbors=18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "# Grid search uses all the parameters\n",
    "\n",
    "param_grid = {'n_neighbors': range(1, 20)}\n",
    "\n",
    "model = GridSearchCV(KNeighborsClassifier(), \n",
    "                    param_grid, \n",
    "                    cv=3, \n",
    "                    scoring='accuracy')\n",
    "\n",
    "model = model.fit(X_train, y_train)\n",
    "\n",
    "print(model.best_params_, model.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Using the best estimator, we can now calculate the error of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8157894736842105\n"
     ]
    }
   ],
   "source": [
    "model = model.best_estimator_\n",
    "\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.38888889, 0.61111111]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.classes_)\n",
    "model.predict_proba([[71,1,2,134,244,1,1,159,1,.9,2,2,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e785390d20a2e9cd68d0819557a872519f720df7cd291d62df73ae0eabc9b696"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pyc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
